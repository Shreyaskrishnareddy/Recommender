import warnings
warnings.filterwarnings('ignore')

!pip install langchain

!pip install openai

!pip install pinecone-client

from langchain.text_splitter import RecursiveCharacterTextSplitter
from openai import OpenAI
from pinecone import Pinecone, ServerlessSpec
from tqdm.auto import tqdm, trange
#from DLAIUtils import Utils

import pandas as pd
import time
import os

# Create or update DLAIUtils.py file
with open('DLAIUtils.py', 'w') as file:
    file.write('''
class Utils:
    def __init__(self):
        # Initialize your class here, if needed
        pass

    def get_pinecone_api_key(self):
        # Replace with your actual method to get the API key
        return "c78d18ee-4be1-429b-8260-5b8ef2d8e18a"

    def get_openai_api_key(self):
        # Replace with your actual method to get the API key
        return "sk-proj-YOUR_OPENAI_PROJECT_KEY_HERE"

    def create_dlai_index_name(self, base_name):
        # Replace with your actual method to create a DLAI index name
        return f"{base_name}-index"
    ''')

# Import the Utils class from the DLAIUtils.py file
from importlib import reload  # Add this import
import DLAIUtils
reload(DLAIUtils)  # Reload the module to reflect changes
from DLAIUtils import Utils

# Create an instance of Utils
utils = Utils()

# Get the API keys
PINECONE_API_KEY = utils.get_pinecone_api_key()
OPENAI_API_KEY = utils.get_openai_api_key()

# Use the create_dlai_index_name method
INDEX_NAME = utils.create_dlai_index_name('dl-ai')

# Print the INDEX_NAME to verify (optional)
print("Index Name:", INDEX_NAME)

# Initialize OpenAI and Pinecone clients
openai_client = OpenAI(api_key=OPENAI_API_KEY)
pinecone = Pinecone(api_key=PINECONE_API_KEY)

# Check if the index exists and delete it if it does
if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:
    pinecone.delete_index(INDEX_NAME)

# Create a new index
pinecone.create_index(name=INDEX_NAME, dimension=1536, metric='cosine',
                      spec=ServerlessSpec(cloud='aws', region='us-east-1'))

# Access the newly created index
index = pinecone.Index(INDEX_NAME)

!wget -q --show-progress -O all-the-news-3.zip "https://www.dropbox.com/scl/fi/wruzj2bwyg743d0jzd7ku/all-the-news-3.zip?rlkey=rgwtwpeznbdadpv3f01sznwxa&dl=1"

!unzip all-the-news-3.zip

with open('/content/all-the-news-3.csv', 'r') as f:
    header = f.readline()
    print(header)

df = pd.read_csv('/content/all-the-news-3.csv', nrows=99)
df.head()

openai_client = OpenAI(api_key=OPENAI_API_KEY)
# util = Utils()  # This variable is not used, so we can remove it
INDEX_NAME = utils.create_dlai_index_name('dl-ai') # Use utils
pinecone = Pinecone(api_key=PINECONE_API_KEY)

if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:
  pinecone.delete_index(INDEX_NAME)

pinecone.create_index(name=INDEX_NAME, dimension=1536, metric='cosine',
  spec=ServerlessSpec(cloud='aws', region='us-east-1'))

index = pinecone.Index(INDEX_NAME)

def get_embeddings(articles, model="text-embedding-ada-002"):
   return openai_client.embeddings.create(input = articles, model=model)

CHUNK_SIZE=400
TOTAL_ROWS=10000
progress_bar = tqdm(total=TOTAL_ROWS)
chunks = pd.read_csv('/content/all-the-news-3.csv', chunksize=CHUNK_SIZE,
                     nrows=TOTAL_ROWS)
chunk_num = 0
for chunk in chunks:
    titles = chunk['title'].tolist()
    embeddings = get_embeddings(titles)
    prepped = [{'id':str(chunk_num*CHUNK_SIZE+i), 'values':embeddings.data[i].embedding,
                'metadata':{'title':titles[i]},} for i in range(0,len(titles))]
    chunk_num = chunk_num + 1
    if len(prepped) >= 200:
      index.upsert(prepped)
      prepped = []
    progress_bar.update(len(chunk))

index.describe_index_stats()

def get_recommendations(pinecone_index, search_term, top_k=10):
  embed = get_embeddings([search_term]).data[0].embedding
  res = pinecone_index.query(vector=embed, top_k=top_k, include_metadata=True)
  return res

reco = get_recommendations(index, 'obama')
for r in reco.matches:
    print(f'{r.score} : {r.metadata["title"]}')

if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:
  pinecone.delete_index(name=INDEX_NAME)

pinecone.create_index(name=INDEX_NAME, dimension=1536, metric='cosine',
  spec=ServerlessSpec(cloud='aws', region='us-east-1'))
articles_index = pinecone.Index(INDEX_NAME)

def embed(embeddings, title, prepped, embed_num):
  for embedding in embeddings.data:
    prepped.append({'id':str(embed_num), 'values':embedding.embedding, 'metadata':{'title':title}})
    embed_num += 1
    if len(prepped) >= 100:
        articles_index.upsert(prepped)
        prepped.clear()
  return embed_num

news_data_rows_num = 100

embed_num = 0 #keep track of embedding number for 'id'
text_splitter = RecursiveCharacterTextSplitter(chunk_size=400,
    chunk_overlap=20) # how to chunk each article
prepped = []
df = pd.read_csv('/content/all-the-news-3.csv', nrows=news_data_rows_num)
articles_list = df['article'].tolist()
titles_list = df['title'].tolist()

for i in range(0, len(articles_list)):
    print(".",end="")
    art = articles_list[i]
    title = titles_list[i]
    if art is not None and isinstance(art, str):
      texts = text_splitter.split_text(art)
      embeddings = get_embeddings(texts)
      embed_num = embed(embeddings, title, prepped, embed_num)

articles_index.describe_index_stats()

reco = get_recommendations(articles_index, 'obama', top_k=100)
seen = {}
for r in reco.matches:
    title = r.metadata['title']
    if title not in seen:
        print(f'{r.score} : {title}')
        seen[title] = '.'